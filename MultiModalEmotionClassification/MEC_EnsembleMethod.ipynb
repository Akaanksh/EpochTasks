{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "audio_folder = \"/content/drive/MyDrive/multimodal_emotion_recognition/data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ9ghqt4tUP1",
        "outputId": "bacce1f8-71ec-456f-a6d7-a28e9b7699e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "q48GFbyHqUge",
        "outputId": "6d1c3be1-feae-46d3-a266-03c7c7a28c76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9df04f385a95>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;31m# Train individual models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9df04f385a95>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmfccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unpack mfcc, spec, label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mmfccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmfccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "mfcc_dir = \"/content/drive/MyDrive/multimodal_emotion_recognition/mfccs\"\n",
        "spec_dir = \"/content/drive/MyDrive/multimodal_emotion_recognition/spectrograms\"\n",
        "\n",
        "# MFCC Dataset\n",
        "class MFCCDataset(Dataset):\n",
        "    def __init__(self, mfcc_dir):\n",
        "        self.mfcc_dir = mfcc_dir\n",
        "        self.files = sorted([f for f in os.listdir(mfcc_dir) if f.endswith('.npy')])\n",
        "        self.label_map = {\n",
        "            \"neutral\": 0, \"calm\": 1, \"happy\": 2, \"sad\": 3,\n",
        "            \"angry\": 4, \"fearful\": 5, \"disgust\": 6, \"surprised\": 7\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file = self.files[idx]\n",
        "        mfcc = np.load(os.path.join(self.mfcc_dir, file))\n",
        "        mfcc = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)\n",
        "        label = self.label_map[file.split(\"_\")[0]]\n",
        "        return mfcc, label\n",
        "\n",
        "# Spectrogram Dataset (sorted)\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, spec_dir, transform=None):\n",
        "        self.spec_files = sorted(glob.glob(os.path.join(spec_dir, \"*.png\")))\n",
        "        self.transform = transform\n",
        "        self.label_map = {\n",
        "            \"neutral\": 0, \"calm\": 1, \"happy\": 2, \"sad\": 3,\n",
        "            \"angry\": 4, \"fearful\": 5, \"disgust\": 6, \"surprised\": 7\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spec_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.spec_files[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.label_map[os.path.basename(img_path).split(\"_\")[0]]\n",
        "        return img, label\n",
        "\n",
        "# Combined Ensemble Dataset\n",
        "class EnsembleDataset(Dataset):\n",
        "    def __init__(self, mfcc_dataset, spec_dataset):\n",
        "        self.mfcc_dataset = mfcc_dataset\n",
        "        self.spec_dataset = spec_dataset\n",
        "        assert len(mfcc_dataset) == len(spec_dataset)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mfcc_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mfcc, label1 = self.mfcc_dataset[idx]\n",
        "        spec, label2 = self.spec_dataset[idx]\n",
        "        assert label1 == label2\n",
        "        return mfcc, spec, label1\n",
        "\n",
        "# Define Models\n",
        "class MFCCCNN(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(MFCCCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 10 * 50, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "class SpectrogramCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpectrogramCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Ensemble Evaluation Logic\n",
        "\n",
        "def evaluate_ensemble(model_mfcc, model_spec, dataloader, device):\n",
        "    model_mfcc.eval()\n",
        "    model_spec.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mfccs, specs, labels in dataloader:\n",
        "            mfccs, specs, labels = mfccs.to(device), specs.to(device), labels.to(device)\n",
        "            out1 = model_mfcc(mfccs)\n",
        "            out2 = model_spec(specs)\n",
        "            outputs = (out1 + out2) / 2\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Training Loop for Individual Models\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for mfccs, specs, labels in dataloader:  # Unpack mfcc, spec, label\n",
        "            mfccs, specs, labels = mfccs.to(device), specs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(mfccs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / total:.4f}, Accuracy: {acc:.2f}%\")\n",
        "\n",
        "\n",
        "# Main Execution Logic\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "mfcc_dataset = MFCCDataset(mfcc_dir)\n",
        "spec_dataset = SpectrogramDataset(spec_dir, transform)\n",
        "ensemble_dataset = EnsembleDataset(mfcc_dataset, spec_dataset)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(ensemble_dataset))\n",
        "val_size = len(ensemble_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(ensemble_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader_mfcc = DataLoader([d[0:2:2] for d in train_dataset], batch_size=32, shuffle=True)\n",
        "train_loader_spec = DataLoader([d[1::2] + (d[2],) for d in train_dataset], batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model_mfcc = MFCCCNN()\n",
        "model_spec = SpectrogramCNN()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_mfcc = torch.optim.Adam(model_mfcc.parameters(), lr=0.001)\n",
        "optimizer_spec = torch.optim.Adam(model_spec.parameters(), lr=0.001)\n",
        "\n",
        "# Train individual models\n",
        "train_model(model_mfcc, train_loader_mfcc, criterion, optimizer_mfcc, device, num_epochs=10)\n",
        "train_model(model_spec, train_loader_spec, criterion, optimizer_spec, device, num_epochs=10)\n",
        "\n",
        "# Evaluate ensemble\n",
        "ensemble_accuracy = evaluate_ensemble(model_mfcc, model_spec, val_loader, device)\n",
        "print(f\"Ensemble Validation Accuracy: {ensemble_accuracy:.2f}%\")\n"
      ]
    }
  ]
}